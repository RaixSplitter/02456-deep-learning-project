{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from typing import Optional,Iterable\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot as f_one_hot\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def compute_confusion_matrix(target, pred, normalize=None):\n",
    "    return metrics.confusion_matrix(\n",
    "        target.detach().cpu().numpy(), \n",
    "        pred.detach().cpu().numpy(),\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "def pkernel_loss(y_pred: torch.Tensor, loss: torch.nn.Module) -> torch.Tensor:\n",
    "    total_loss = 0.\n",
    "    B = y_pred.shape[0]\n",
    "    n_classes = y_pred.shape[-1]\n",
    "    for i in range(n_classes):\n",
    "        y_target = torch.zeros_like(y_pred).scatter(1, torch.tensor([i], dtype=torch.long, device=y_pred.device).repeat(B).unsqueeze(1), 1)\n",
    "        total_loss += loss(y_pred.float(), y_target)\n",
    "    total_loss = total_loss / n_classes**0.5\n",
    "    return total_loss\n",
    "\n",
    "def get_gradient(model: torch.nn.Module, x: torch.Tensor, loss_fn: callable, opt: torch.optim.Optimizer, flatten: bool = False, use_label: bool = False, y: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the gradient of each element of the batch in x of the model with respect to the loss function\n",
    "    \n",
    "    Args:\n",
    "        model: the model to use\n",
    "        x: the input data [N_samples, ...]\n",
    "        y: the target data [N_samples, ...]\n",
    "        loss_fn: the loss function to use\n",
    "        opt: the optimizer to use\n",
    "        positive: whether to perturb the loss function positively or negatively\n",
    "        kernel: What type of kernel to use, can either be pKernel, direct or average\n",
    "        \n",
    "    Returns:\n",
    "        grads: the gradients of the model with respect to the loss function\n",
    "    \"\"\"\n",
    "    B = len(x)\n",
    "    device = next(model.parameters()).device\n",
    "    x = x.to(device)\n",
    "    if y is not None:\n",
    "        y = y.to(device).float()\n",
    "    opt.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    if use_label:\n",
    "        y_target = y\n",
    "    else:\n",
    "        y_target = y_pred.detach().argmax(1)\n",
    "        y_target = torch.zeros_like(y_pred).scatter(1, y_target.unsqueeze(1), 1)\n",
    "       \n",
    "    loss = pkernel_loss(y_pred, loss_fn)\n",
    "    # print(loss)\n",
    "    # Trick to get gradient with respect to each sample in parallel\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), is_grads_batched=True, grad_outputs=torch.eye(B).to(device))\n",
    "    if flatten:\n",
    "        grads = torch.cat([grad.view((B, -1)) for grad in grads], -1)\n",
    "    return grads\n",
    "\n",
    "class own_linear_layer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(own_linear_layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.beta = 0.1\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Initialize the weights to normal distribution\n",
    "        nn.init.normal_(self.weight, mean=0.0, std=1.0)\n",
    "        if self.bias is not None:\n",
    "            nn.init.normal_(self.bias, mean=0.0, std=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.t()/(self.weight.shape[-1]**0.5) + self.beta*self.bias\n",
    "\n",
    "class SingleLayerMLP(nn.Module):\n",
    "    \"\"\" A simple single hidden-layer perceptron for MNIST classification \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, num_layers: int = 1):\n",
    "        super(SingleLayerMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(own_linear_layer(input_size, hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(own_linear_layer(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(own_linear_layer(hidden_size, output_size))\n",
    "        \n",
    "        # Initialize weights\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, own_linear_layer):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Flatten input\n",
    "        x = x.view(-1, self.input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [layer.weight for layer in self.layers if isinstance(layer, own_linear_layer)]\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return [layer.bias for layer in self.layers if isinstance(layer, own_linear_layer)]\n",
    "    \n",
    "    def set_weights(self, weights, biases, initial_gain):\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, own_linear_layer):\n",
    "                weight = weights[i]\n",
    "                bias = biases[i]\n",
    "                in_features = layer.in_features\n",
    "                out_features = layer.out_features\n",
    "                k_factor = initial_gain\n",
    "                layer.weight.data = weight.data[:out_features, :in_features]*k_factor\n",
    "                layer.bias.data = bias.data[:out_features]*k_factor\n",
    "                i += 1\n",
    "    \n",
    "\n",
    "class GaussianFit(torch.nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module, classes, device: torch.device, kernel_method: str = \"direct\", noise_var: float = 0.0):\n",
    "        super(GaussianFit, self).__init__()\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.kernel_method = kernel_method\n",
    "        self.noise_var = noise_var\n",
    "        self.covariance_matrix = None\n",
    "        self.optimizer = None\n",
    "        self.classes = classes\n",
    "        \n",
    "    def fit(self, data: Iterable[torch.Tensor], optimizer: torch.optim.Optimizer, loss_batched: torch.nn.Module):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss_batched\n",
    "        xs, ys, y_hats = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in data:\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "                y_hats.append(self.model(x.to(self.device)))\n",
    "        xs    = torch.cat(xs, 0).to(self.device)\n",
    "        y     = torch.cat(ys, 0).to(self.device)\n",
    "        y_hat = torch.cat(y_hats, 0).to(self.device)\n",
    "        self.label_diff = f_one_hot(y[:,-1],self.classes) - y_hat\n",
    "        self.grads = get_gradient(self.model, xs, loss_batched, self.optimizer, True, True, y=y)\n",
    "        self.update_w()\n",
    "        \n",
    "    def update_noise(self, noise_var: float):\n",
    "        self.noise_var = noise_var\n",
    "        self.update_w()\n",
    "        \n",
    "    def update_w(self):\n",
    "        self.covarinace_kernel = self.grads@self.grads.T\n",
    "        self.covariance_matrix = self.covarinace_kernel.clone()\n",
    "        self.covariance_matrix[range(self.covariance_matrix.shape[0]), range(self.covariance_matrix.shape[0])] += self.noise_var\n",
    "        self.W = torch.linalg.solve(self.covariance_matrix, self.label_diff).to(self.device)\n",
    "        \n",
    "    def encode_x(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Function transforming input x into the gradient kernel space \"\"\"\n",
    "        x_grad = get_gradient(self.model, x, self.loss, self.optimizer, True, True)\n",
    "        return x_grad @ self.grads.T\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_grad = get_gradient(self.model, x, self.loss, self.optimizer, True, True)\n",
    "        K_xX = x_grad@self.grads.T\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)\n",
    "        return y_hat + K_xX @ self.W\n",
    " \n",
    "def MSELoss_batch(y_hat, y):\n",
    "    return 0.5*(y_hat-y).pow(2).sum(-1)\n",
    "\n",
    "\n",
    "def decision_boundary(x: torch.Tensor, threshold: float, flip_chance: float = 0) -> torch.Tensor:\n",
    "    y = ((x[:, 0] > threshold) | (x[:, 1] > threshold)).float()\n",
    "    should_flip = torch.rand(y.size()) < flip_chance\n",
    "    y[should_flip] = 1 - y[should_flip]\n",
    "    return y\n",
    "\n",
    "\n",
    "def sample_data(n: int, threshold: float = 0.5**0.5, seed: Optional[int] = None, flip_chance: float = 0) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    x = torch.rand(n, 2)\n",
    "    y = decision_boundary(x, threshold, flip_chance)\n",
    "    return x, y\n",
    "\n",
    "def plot_decision_boundary(model: torch.nn.Module, data, fig, ax, title = None) -> None:\n",
    "    x_train   = data[\"x_train\"]\n",
    "    y_train   = data[\"y_train\"]\n",
    "    x_test    = data[\"x_test\"]\n",
    "    x_grid    = data[\"x_grid\"]\n",
    "    N_testing = data[\"N_testing\"]\n",
    "    threshold = data[\"threshold\"]\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = torch.stack([model.forward(x_test_batch) for x_test_batch in torch.split(x_test, 100)]).view(-1)\n",
    "    y_acc = (y_hat >= 0.5)== ((x_test[:, 0] > threshold) | (x_test[:, 1] > threshold))\n",
    "    ax.clear()\n",
    "    \n",
    "    y_hat_reshape = y_hat.view(int(N_testing**0.5), int(N_testing**0.5)).detach().numpy().reshape(int(N_testing**0.5), int(N_testing**0.5))\n",
    "    # Plot decision boundary\n",
    "    # Color blue if y_hat > 0.5, red if y_hat < 0.5 by making a colormap of two colors\n",
    "    colors = [\"tab:red\", \"tab:blue\"]\n",
    "    custom_cmap = plt.cm.colors.ListedColormap(colors)\n",
    "    ax.contourf(*x_grid, y_hat_reshape >0.5, alpha=0.5, levels=torch.linspace(-5.5, 5.5, 3), cmap=custom_cmap)\n",
    "     \n",
    "    # Plot training data\n",
    "    ax.scatter(x_train[y_train == 0, 0], x_train[y_train == 0, 1], c='r', label='Class 0', s=4, marker='x')\n",
    "    ax.scatter(x_train[y_train == 1, 0], x_train[y_train == 1, 1], c='b', label='Class 1', s=4, marker='x')\n",
    "    # Add title\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        ax.set_title('Accuracy: {:.2f}'.format(y_acc.float().mean().item()))\n",
    "    ax.set_xlabel('$x_0$')\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    \n",
    "    # Add striped line to mark the decision boundary (x0 < threshold and x1 < threshold)\n",
    "    ax.plot([0, threshold], [threshold, threshold], 'k--')\n",
    "    ax.axvline(x=threshold, color='k', linestyle='--', ymax=threshold, label='Correct Decision boundary')\n",
    "    ax.legend(loc='upper center')\n",
    "    model.train()\n",
    "    \n",
    "def plot_NTK_decision_boundary(model, data, noise_var: float = 0.0, ax: Optional[plt.Axes] = None, fig: Optional[plt.Figure] = None, title = None):\n",
    "    n_hidden = data.get(\"n_hidden\", 64)\n",
    "    assert isinstance(n_hidden, int)\n",
    "    gradient_loader = data[\"gradient_loader\"]\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
    "    kernel_model = GaussianFit(model, \"cpu\", noise_var=noise_var)\n",
    "    kernel_model.fit(gradient_loader, optimizer, MSELoss_batch)\n",
    "    plot_decision_boundary(kernel_model, data, fig, ax, title)\n",
    "    plt.show()\n",
    "\n",
    "def NTK_model(model, data, noise_var: float = 0.0):\n",
    "    gradient_loader = data[\"gradient_loader\"]\n",
    "    device          = data[\"device\"]\n",
    "    classes         = data[\"classes\"]\n",
    "    optimizer    = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
    "    kernel_model = GaussianFit(model, classes, device, noise_var=noise_var)\n",
    "    kernel_model.fit(gradient_loader, optimizer, MSELoss_batch)\n",
    "    # kernel_model.fit(gradient_loader, optimizer, nn.CrossEntropyLoss(reduction=\"none\"))\n",
    "    \n",
    "    return kernel_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### START OF DEMO #####\n",
    "# # Setup data\n",
    "# threshold   = 0.5**0.5\n",
    "# flip_chance = 0.0\n",
    "# N_training  = 100\n",
    "# N_testing   = 10**4\n",
    "# x_train, y_train = sample_data(N_training, seed=0, flip_chance=flip_chance, threshold=threshold)\n",
    "# gradient_loader = DataLoader([(x_train[i], y_train[i, None]) for i in range(N_training)], batch_size=64, shuffle=False)\n",
    "\n",
    "# # Construct grid of sqrt(N_testing) x sqrt(N_testing) points\n",
    "# x = torch.linspace(0, 1, int(N_testing**0.5))\n",
    "# x_grid = torch.meshgrid(x, x, indexing='ij')\n",
    "# x_test = torch.stack([x_grid[0].flatten(), x_grid[1].flatten()], 1)\n",
    "# y_test = decision_boundary(x_test, threshold)\n",
    "# test_set = TensorDataset(x_test,y_test)\n",
    "# target_loader   = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# # Construct NN_model\n",
    "# n_in = 2\n",
    "# n_out = 1\n",
    "# n_hidden = 64*128\n",
    "# model_arch = SingleLayerMLP\n",
    "# criterion = lambda x, y: ((x-y)**2).mean()  # We need L2 loss for NTK\n",
    "# train_model = model_arch(n_in, n_out, n_hidden)\n",
    "# optimizer = torch.optim.SGD(train_model.parameters(), lr=1., momentum=0.9)\n",
    "\n",
    "# # combine into one data\n",
    "# data = {\n",
    "#     \"threshold\"       : threshold, \n",
    "#     \"flip_chance\"     : flip_chance,\n",
    "#     \"N_training\"      : N_training,\n",
    "#     \"N_testing\"       : N_testing,\n",
    "#     \"x_train\"         : x_train,\n",
    "#     \"x_test\"          : x_test,\n",
    "#     \"y_train\"         : y_train,\n",
    "#     \"y_test\"          : y_test,\n",
    "#     \"x_grid\"          : x_grid,\n",
    "#     \"n_in\"            : n_in,\n",
    "#     \"n_out\"           : n_out,\n",
    "#     \"n_hidden\"        : n_hidden,\n",
    "#     \"gradient_loader\" : gradient_loader,\n",
    "# }\n",
    "\n",
    "# # Train network to compare decision boundaries \n",
    "# train_model.train()\n",
    "# for i in tqdm(range(10000)):\n",
    "#     for x, y in gradient_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         y_hat = train_model(x)\n",
    "#         # print(y_hat.shape,y.shape)\n",
    "#         loss = ((y_hat - y.float())**2).mean()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         break\n",
    "    \n",
    "# # Plot decision boundary\n",
    "# fig, ax = plt.subplots(2, 1)\n",
    "# fig.set_size_inches(5, 10)\n",
    "# plot_decision_boundary(train_model, data, fig, ax[0])#, 'Trained NN')\n",
    "\n",
    "# plot_NTK_decision_boundary(train_model, data, 0, ax[1], fig, 'Corresponding Gaussian Process')\n",
    "# fig.tight_layout()\n",
    "# fig.patch.set_alpha(0)  # Make background transparent\n",
    "\n",
    "# # Save figure\n",
    "# fig.savefig('decision_boundary_comparison.png')\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluate\n",
    "# valid = []\n",
    "# train_model.eval()\n",
    "# for x, y in target_loader:\n",
    "#     y_hat = train_model(x).view(-1)\n",
    "#     acc = (y_hat >= 0.5) == ((x[:, 0] > threshold) | (x[:, 1] > threshold))\n",
    "#     valid.append(acc)\n",
    "# print(torch.cat(valid).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train_set = MNIST(\"../../3_Feedforward_PyTorch/temp/\", train=True,  download=False, transform=transforms.ToTensor())\n",
    "test_set  = MNIST(\"../../3_Feedforward_PyTorch/temp/\", train=False, download=False, transform=transforms.ToTensor())\n",
    "batch_size = 32 # both for training and testing\n",
    "\n",
    "# # use only 1000 train and 1000 test\n",
    "train_loader = DataLoader([(train_set.data[i].float()/255, train_set.targets[i,None]) for i in range(1000)], batch_size=batch_size, shuffle=True,  num_workers=0, drop_last=False)\n",
    "test_loader  = DataLoader([( test_set.data[i].float()/255,  test_set.targets[i,None]) for i in range(1000)], batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "#--- Limit for testing ---\n",
    "# train_label_size = 600\n",
    "# test_label_size  = 300\n",
    "# train_ta = train_set.data[train_set.targets == 0][:train_label_size].float()\n",
    "# train_tb = train_set.data[train_set.targets == 1][:train_label_size].float()\n",
    "\n",
    "# train_la = train_set.targets[train_set.targets == 0][:train_label_size]\n",
    "# train_lb = train_set.targets[train_set.targets == 1][:train_label_size]\n",
    "# train_la[:] = 0\n",
    "# train_lb[:] = 1\n",
    "\n",
    "# x_train = torch.cat((train_ta,train_tb))\n",
    "# y_train = torch.cat((train_la,train_lb))\n",
    "\n",
    "# # test\n",
    "# test_ta = test_set.data[test_set.targets == 0][:test_label_size].float()\n",
    "# test_tb = test_set.data[test_set.targets == 1][:test_label_size].float()\n",
    "\n",
    "# test_la = test_set.targets[test_set.targets == 0][:test_label_size]\n",
    "# test_lb = test_set.targets[test_set.targets == 1][:test_label_size]\n",
    "# test_la[:] = 0\n",
    "# test_lb[:] = 1\n",
    "\n",
    "# x_test = torch.cat((test_ta,test_tb))\n",
    "# y_test = torch.cat((test_la,test_lb))\n",
    "\n",
    "# train_loader = DataLoader([(x_train[i].float()/255, y_train[i, None]) for i in range(train_label_size)], batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "# test_loader  = DataLoader([(x_test[i].float()/255,  y_test[i, None])  for i in range(test_label_size)], batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "#--- Limit for testing ---\n",
    "\n",
    "# plot a few MNIST examples\n",
    "idx, dim, classes = 0, 28, 10\n",
    "images, labels = next(iter(train_loader))\n",
    "img = images.detach().cpu()\n",
    "\n",
    "# # create empty canvas\n",
    "# canvas = np.zeros((dim*2, 8*dim))\n",
    "\n",
    "# # fill with tensors\n",
    "# for i in range(2):\n",
    "#     for j in range(8):\n",
    "#         canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = img[idx].reshape((dim, dim))\n",
    "#         idx += 1\n",
    "\n",
    "# # visualize matrix of tensors as gray scale image\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.axis('off')\n",
    "# plt.imshow(canvas, cmap='gray')\n",
    "# plt.title('MNIST handwritten digits')\n",
    "# plt.show()\n",
    "# print(len(train_loader.dataset),len(test_loader.dataset))\n",
    "# print(images[0].size(),labels.size())\n",
    "\n",
    "# fig,ax = plt.subplots(2,8,figsize=(20,6))\n",
    "# ax = ax.flat\n",
    "# for i in range(len(ax)):\n",
    "#     ax[i].imshow(img[i], cmap=\"gray\")\n",
    "#     ax[i].set_title(labels[i].item())\n",
    "#     ax[i].axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SingleLayerMLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): own_linear_layer()\n",
      "    (1): ReLU()\n",
      "    (2): own_linear_layer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "N_training = len(train_loader.dataset)\n",
    "N_testing  = len(test_loader.dataset)\n",
    "\n",
    "# Construct NN_model\n",
    "n_in = dim**2\n",
    "n_out = classes\n",
    "n_hidden = 2*512 # 64*128\n",
    "model_arch = SingleLayerMLP\n",
    "criterion = lambda x, y: ((x-y)**2).mean()  # We need L2 loss for NTK\n",
    "train_model = model_arch(n_in, n_out, n_hidden, num_layers=1)\n",
    "optimizer = torch.optim.SGD(train_model.parameters(), lr=1.00, momentum=0.9)\n",
    "CELoss    = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device('cuda')  \n",
    "else:                         device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device:\",device)\n",
    "train_model.to(device)\n",
    "print(train_model)\n",
    "\n",
    "data = {\n",
    "    \"N_training\"      : N_training,\n",
    "    \"N_testing\"       : N_testing,\n",
    "    \"n_in\"            : n_in,\n",
    "    \"n_out\"           : n_out,\n",
    "    \"n_hidden\"        : n_hidden,\n",
    "    \"gradient_loader\" : train_loader,\n",
    "    \"device\"          : device,\n",
    "    \"classes\"         : classes,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cd072521b74a15ab51cad68d2d1026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.67\n",
      "80.95\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "# Train network to compare decision boundaries \n",
    "\n",
    "train_model.train()\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "valid_acc  = []\n",
    "for i in (pbar := tqdm(range(epochs))):\n",
    "    train_loss_batches = []\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = train_model(inputs)\n",
    "        # loss = CELoss(output, targets[:,-1])\n",
    "        loss = ((output - nn.functional.one_hot(targets[:,-1],classes).float())**2).mean() / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = output.max(1)[1]\n",
    "        train_loss_batches.append(loss.detach().cpu())\n",
    "    train_loss.append(np.mean(train_loss_batches))\n",
    "    \n",
    "    # Compute accuracies on validation set.\n",
    "    valid_loss_batches = []\n",
    "    valid_acc_batches = []\n",
    "    with torch.no_grad():\n",
    "        train_model.eval()\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = train_model(inputs)\n",
    "            \n",
    "            # loss        = CELoss(output, targets[:,-1])\n",
    "            loss = ((output - nn.functional.one_hot(targets[:,-1],classes).float())**2).mean() / 2\n",
    "            predictions = output.max(1)[1]\n",
    "\n",
    "            # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "            valid_loss_batches.append(loss.cpu())\n",
    "            valid_acc_batches.append(accuracy(targets, predictions))\n",
    "\n",
    "        train_model.train()\n",
    "\n",
    "    valid_loss.append(np.mean(valid_loss_batches))\n",
    "    valid_acc.append(np.mean(valid_acc_batches))\n",
    "print(round(valid_acc[-1]*100,2))\n",
    "\n",
    "# NTK model\n",
    "NTK_valid_loss_batches = []\n",
    "NTK_valid_acc_batches  = []\n",
    "kernel_model = NTK_model(train_model, data)\n",
    "kernel_model.eval()\n",
    "for inputs, targets in test_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    output = kernel_model.forward(inputs)\n",
    "    \n",
    "    predictions = output.max(1)[1]\n",
    "    \n",
    "    NTK_valid_acc_batches.append(accuracy(targets, predictions))\n",
    "\n",
    "print(round(np.mean(NTK_valid_acc_batches)*100,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAE/CAYAAAB1vdadAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAAsTAAALEwEAmpwYAAAv0klEQVR4nO3deZwdZZ3v8c83vaSzrx0g6aQTBZWgCJk2bggBXECRiIISN1CvqCPjeJXLgIzAMPJyY5RxZBy5AyKiRgYdb9AgKgmCjmiaAJEQoxEh6Sykk5ClE7J08rt/VHU46XSnT9LndJ3l+3696tV1nnrqnF8lUPl21XOeUkRgZmZmZoU1KOsCzMzMzCqRQ5aZmZlZEThkmZmZmRWBQ5aZmZlZEThkmZmZmRWBQ5aZmZlZEThkmZmZmRWBQ5YVjaSnJL0+6zrMzCTdL+lZSYOzrsWqh0OWmZlVNElTgdcBAZw7gJ9bO1CfZaXJIcsGlKTBkm6UtCZdbuz6zVLSeEk/kbRZ0iZJD0oalG77B0mrJW2TtFzSmdkeiZmVkfcDDwG3ARd1NUqaLOlHktolbZT09ZxtH5a0LD3nPCFpRtoeko7N6XebpM+l67MktaXnq3XAtySNSc9r7emVtJ9IasrZf6ykb6Xnw2cl/Thtf1zSW3P61UnaIOnkYv0hWeE5ZNlAuwp4FXAS8HJgJvCP6bZPA21AI3AU8BkgJL0YuBR4RUSMAN4EPDWgVZtZOXs/8N10eZOkoyTVAD8BngamApOAuQCSLgCuTfcbSXL1a2Oen3U0MBZoBi4h+Xf2W+nrKcBzwNdz+n8HGAqcAEwAvpq23w68N6ffm4G1EfFInnVYCfClTBto7wH+LiLWA0j6J+CbwGeBPcAxQHNErAAeTPvsBQYD0yW1R8RTWRRuZuVH0ikkAefOiNgg6S/Au0mubE0E/k9EdKbdf53+/F/AlyJiUfp6xWF85D7gmojYlb5+DvhhTj3XAwvT9WOAs4FxEfFs2uVX6c87gM9KGhkRW4H3kQQyKyO+kmUDbSLJb45dnk7bAL5McjL7uaQnJV0BkAauT5L8Zrle0lxJEzEz69tFwM8jYkP6+ntp22Tg6ZyAlWsy8Jcj/Lz2iNjZ9ULSUEnflPS0pK3AA8Do9EraZGBTTsDaLyLWAL8B3iFpNEkY++4R1mQZcciygbaG5LfKLlPSNiJiW0R8OiJeQHJ5/lNdY68i4nsR0fUbaQBfHNiyzazcSBoCvBM4TdK6dJzU/yYZqvAMMKWXwemrgBf28rY7SG7vdTm62/bo9vrTwIuBV0bESODUrvLSzxmbhqiefJvkluEFwG8jYnUv/axEOWRZsdVJauhagO8D/yipUdJ44GqSy+JIOkfSsZIEbAH2AvskvVjSGekA+Z0kl9/3ZXM4ZlZG3kZyHplOMg70JOB4kqEIbwPWAl+QNCw9R7023e8/gcsk/Y0Sx0rq+uXwUeDdkmoknQWc1kcNI0jOWZsljQWu6doQEWuBe4B/TwfI10k6NWffHwMzgL8nGaNlZcYhy4ptPskJpmtpAFqBJcAfgMXA59K+xwG/BDqA3wL/HhELScZjfQHYAKwjGRx65cAdgpmVqYuAb0XEyohY17WQDDyfA7wVOBZYSfKlm3cBRMR/AdeT3FrcRhJ2xqbv+ffpfptJxpj+uI8abgSGkJy/HgJ+1m37+0jGo/4RWE8yNIK0jq7xXNOAH+V/2FYqFNH9yqaZmZmVAklXAy+KiPf22dlKjr9daGZmVoLS24sfIrnaZWXItwvNzMxKjKQPkwyMvyciHsi6Hjsyvl1oZmZmVgS+kmVmZmZWBA5ZZmZmZkVQcgPfx48fH1OnTs26DDMbQA8//PCGiGjMuo5C8DnMrLoc6vxVciFr6tSptLa2Zl2GmQ0gSU/33as8+BxmVl0Odf7y7UIzMzOzInDIMjMzMysChywzMzOzIii5MVlm1WbPnj20tbWxc+fOrEspuoaGBpqamqirq8u6FDOzonPIMstYW1sbI0aMYOrUqUjKupyiiQg2btxIW1sb06ZNy7ocM7Oi8+1Cs4zt3LmTcePGVXTAApDEuHHjquKKnZkZOGSZlYRKD1hdquU4zcwgz5Al6SxJyyWtkHRFD9tPlbRYUqek83vYPlJSm6SvF6JoMyucjRs3ctJJJ3HSSSdx9NFHM2nSpP2vd+/efch9W1tb+cQnPjFAlZqZlZc+x2RJqgFuAt4AtAGLJM2LiCdyuq0ELgYu6+Vt/hnwU8TNStC4ceN49NFHAbj22msZPnw4l132/P/KnZ2d1Nb2fKpoaWmhpaVlIMo0Mys7+VzJmgmsiIgnI2I3MBeYndshIp6KiCXAvu47S/ob4Cjg5wWo90B33w0/+UnB39as2l188cV89KMf5ZWvfCWXX345v//973n1q1/NySefzGte8xqWL18OwP33388555wDJAHtgx/8ILNmzeIFL3gBX/va17I8BDOzzOXz7cJJwKqc123AK/N5c0mDgH8B3gu8/rCr68uXvgQ1NZCe5M2scNra2vif//kfampq2Lp1Kw8++CC1tbX88pe/5DOf+Qw//OEPD9rnj3/8IwsXLmTbtm28+MUv5mMf+5inazCzqlXsKRz+FpgfEW2HGvAq6RLgEoApU6bk/+7NzfCb3/SzRLMS8slPQnrrrmBOOgluvPGwd7vggguoqakBYMuWLVx00UX8+c9/RhJ79uzpcZ+3vOUtDB48mMGDBzNhwgSeeeYZmpqa+lG8mVn5yud24Wpgcs7rprQtH68GLpX0FHAD8H5JX+jeKSJujoiWiGhpbOzxQdY9a26GtjbYuzf/fcwsL8OGDdu//tnPfpbTTz+dxx9/nLvvvrvXaRgGDx68f72mpobOzs6i12lmVqryuZK1CDhO0jSScHUh8O583jwi3tO1LulioCUiDvp24hGbMgU6O2HNGpg8ue/+ZqXuCK44DYQtW7YwadIkAG677bZsizEzKxN9XsmKiE7gUuBeYBlwZ0QslXSdpHMBJL1CUhtwAfBNSUuLWfR+zc3Jz6efHpCPM6tWl19+OVdeeSUnn3yyr06ZmeVJEZF1DQdoaWmJ1tbW/DovWwbTp8Mdd8B73tN3f7MStGzZMo4//visyxgwPR2vpIcjoiLmgjisc5iZlb1Dnb/Ke8b3rkHyK1dmW4eZmZlZN+UdsoYNg/HjfbvQzMzMSk55hyxIxmU5ZJmZmVmJccgyMzMzK4LKCVklNoDfzMzMqlv5h6wpU2DHDti4MetKzMzMzPYr/5DlubLM+uX000/n3nvvPaDtxhtv5GMf+1iP/WfNmkXXFAVvfvOb2bx580F9rr32Wm644YaC12pmVk4cssyq3Jw5c5g7d+4BbXPnzmXOnDl97jt//nxGjx5dpMrMzMpb5YQsz5VldkTOP/98fvrTn7J7924AnnrqKdasWcP3v/99WlpaOOGEE7jmmmt63Hfq1Kls2LABgOuvv54XvehFnHLKKSxfvnzA6jczK1XlH7LGjk3my/KVLLMjMnbsWGbOnMk999wDJFex3vnOd3L99dfT2trKkiVL+NWvfsWSJUt6fY+HH36YuXPn8uijjzJ//nwWLVo0UOWbmZWsfB4QXdokT+NgFeOTn4RHHy3se550Ut/Pne66ZTh79mzmzp3LLbfcwp133snNN99MZ2cna9eu5YknnuDEE0/scf8HH3yQ8847j6FDhwJw7rnnFvYgzMzKUPlfyQKHLLN+mj17Nvfddx+LFy9mx44djB07lhtuuIH77ruPJUuW8Ja3vIWdO3dmXaaZWVkp/ytZkISs3/8+6yrM+q2vK07FMnz4cE4//XQ++MEPMmfOHLZu3cqwYcMYNWoUzzzzDPfccw+zZs3qdf9TTz2Viy++mCuvvJLOzk7uvvtuPvKRjwzcAZiZlaDKCFlTpiTzZG3fnozPMrPDNmfOHM477zzmzp3LS17yEk4++WRe8pKXMHnyZF772tcect8ZM2bwrne9i5e//OVMmDCBV7ziFQNUtZlZ6aqMkJU7jcP06dnWYlam3va2txE5T0647bbbeux3//33719/6qmn9q9fddVVXHXVVUWqzsys/FTOmCzwuCwzO4ikWyWtl/R4L9sl6WuSVkhaImlGt+0jJbVJ+vrAVGxmlaKyQpbnyjKzg90GnHWI7WcDx6XLJcA3um3/Z+CBolRmZhWtMkLWMcdAba2vZJnZQSLiAWDTIbrMBm6PxEPAaEnHAEj6G+Ao4OfFr9TMKk1lhKyaGpg82SHLylbuWKhKVqLHOQlYlfO6DZgkaRDwL8BlmVRlZmWvMkIWeK4sK1sNDQ1s3LixVANIwUQEGzdupKGhIetS8vW3wPyIaOuro6RLJLVKam1vbx+A0sysHFTGtwshCVn33Zd1FWaHrampiba2NqrhH+eGhgaampqyLqO71cDknNdNadurgddJ+ltgOFAvqSMiruj+BhFxM3AzQEtLS2WnZTPLW+WErClTYM0a2LMH6uqyrsYsb3V1dUybNi3rMqrZPOBSSXOBVwJbImIt8J6uDpIuBlp6ClhmZr2pnJDV3Az79kFbG/gfLDNLSfo+MAsYL6kNuAaoA4iI/wDmA28GVgA7gA9kU6mZVZrKClmQjMtyyDKzVETM6WN7AB/vo89tJFNBmJnlrbIGvoPnyjIzM7OSUDkha3I6btXfMDQzM7MSUDkhq6EBjj7aIcvMzMxKQl4hS9JZkpanz/Y66Ns1kk6VtFhSp6Tzc9pPkvRbSUvTZ4K9q5DFH8RzZZmZmVmJ6DNkSaoBbiJ5vtd0YI6k6d26rQQuBr7XrX0H8P6IOIHk2WE3Shrdz5p755BlZmZmJSKfK1kzgRUR8WRE7Abmkjzra7+IeCoilgD7urX/KSL+nK6vAdYDjQWpvCdTpiQD3/ft67uvmZmZWRHlE7J6fK7X4X6QpJlAPfCXw903b83NsGsXrF9ftI8wMzMzy8eADHxPn2j/HeADEXHQZaaCPfcrd64sMzMzswzlE7J6e65XXiSNBH4KXBURD/XUJyJujoiWiGhpbOzH3UTPlWVmZmYlIp+QtQg4TtI0SfXAhSTP+upT2v+/gdsj4q4jLzNPvpJlZmZmJaLPkBURncClwL3AMuDOiFgq6TpJ5wJIekX6TLALgG9KWpru/k7gVOBiSY+my0nFOBAARo1KFocsMzMzy1hezy6MiPkkD1HNbbs6Z30RyW3E7vvdAdzRzxoPj6dxMDMzsxJQOTO+d3HIMjMzsxLgkGVmZmZWBJUXsqZMgS1bksXMzMwsI5UXsvwNQzMzMysBlRuyPFeWmZmZZahyQ5avZJmZmVmGKi9kTZgAgwc7ZJmZmVmmKi9kDRqUDH53yDIzM7MMVV7IAk/jYGZmZplzyDIzMzMrgsoMWVOmwLp1sHNn1pWYmZlZlarMkNX1DcNVq7Ktw8zMzKpWZYcsz5VlZmZmGanskOVxWWZmZpaRygxZTU3JVA4OWWZmZpaRygxZdXUwcaJDlpmZmWWmMkMWeBoHMzMzy5RDlpmZmVkRVG7ImjIlmcJh796sKzEzM7MqVLkhq7kZOjth7dqsKzEzM7MqVNkhCzxXlpmZmWWi8kOWx2WZmZlZBhyyzMzMzIqgckPWsGEwbpxDlpmZmWWickMWeBoHMzMzy4xDlpmZmVkRVHbImjIlCVkRWVdiZmZmVaayQ1ZzM2zfDps2ZV2JmZmZVZm8QpaksyQtl7RC0hU9bD9V0mJJnZLO77btIkl/TpeLClV4XjxXlpmZmWWkz5AlqQa4CTgbmA7MkTS9W7eVwMXA97rtOxa4BnglMBO4RtKY/pedJ0/jYFb1JN0qab2kx3vZLklfS3+JXCJpRtp+kqTfSlqatr9rYCs3s3KXz5WsmcCKiHgyInYDc4HZuR0i4qmIWALs67bvm4BfRMSmiHgW+AVwVgHqzo9DlpnBbRz6vHM2cFy6XAJ8I23fAbw/Ik5I979R0ujilWlmlSafkDUJWJXzui1ty0d/9u2/ceNg6FCHLLMqFhEPAIcamDkbuD0SDwGjJR0TEX+KiD+n77EGWA80Fr9iM6sUJTHwXdIlkloltba3txfyjT2Ng5n1pc9fBiXNBOqBv/T0BkU7h5lZWcsnZK0GJue8bkrb8pHXvhFxc0S0RERLY2OBf1F0yDKzfpB0DPAd4AMR0X1IBFDkc5iZla18QtYi4DhJ0yTVAxcC8/J8/3uBN0oakw54f2PaNnC65soyM+tZr78MShoJ/BS4Kr2VaGaWtz5DVkR0ApeShKNlwJ0RsVTSdZLOBZD0CkltwAXANyUtTffdBPwzSVBbBFyXtg2c5mbYsCGZL8vM7GDzgPen3zJ8FbAlItamv1T+N8l4rbuyLdHMylFtPp0iYj4wv1vb1Tnri0h+++tp31uBW/tRY/90fcNw1Sp4yUsyK8PMsiHp+8AsYHz6y+A1QB1ARPwHybntzcAKkm8UfiDd9Z3AqcA4SRenbRdHxKMDVbuZlbe8QlZZy53GwSHLrOpExJw+tgfw8R7a7wDuKFZdZlb5SuLbhUXlubLMzMwsA5UfsiZOhNpahywzMzMbUJUfsmpqoKnJIcvMzMwGVOWHLPBcWWZmZjbgqiNkea4sMzMzG2DVEbKam2H1atizJ+tKzMzMrEpUT8jatw/WrMm6EjMzM6sS1ROywLcMzczMbMA4ZJmZmZkVQXWErClTkp8OWWZmZjZAqiNkNTTAUUc5ZJmZmdmAqY6QBZ4ry8zMzAZU9YQsz5VlZmZmA6h6QlZzM6xcCRFZV2JmZmZVoLpC1s6d0N6edSVmZmZWBaorZIFvGZqZmdmAcMgyMzMzKwKHLDMzM7MiqJ6QNXo0jBzpkGVmZmYDonpCFniuLDMzMxsw1RWyPFeWmZmZDZDqClm+kmVmZmYDpPpC1ubNsHVr1pWYmZlZhau+kAXJzO9mZmZmRVSdIcu3DM3MzKzIHLLMzMzMiiCvkCXpLEnLJa2QdEUP2wdL+kG6/XeSpqbtdZK+LekPkpZJurLA9R+eo46C+nqHLDMzMyu6PkOWpBrgJuBsYDowR9L0bt0+BDwbEccCXwW+mLZfAAyOiJcBfwN8pCuAZWLQIE/jYGZmZgMinytZM4EVEfFkROwG5gKzu/WZDXw7Xb8LOFOSgACGSaoFhgC7gWy/2udpHMzMzGwA5BOyJgGrcl63pW099omITmALMI4kcG0H1gIrgRsiYlM/a+4fX8kyMzOzAVDsge8zgb3ARGAa8GlJL+jeSdIlkloltba3txe3ouZmWLsWdu0q7ueYmZlZVcsnZK0GJue8bkrbeuyT3hocBWwE3g38LCL2RMR64DdAS/cPiIibI6IlIloaGxsP/ygOR9c3DNvaivs5ZmZmVtXyCVmLgOMkTZNUD1wIzOvWZx5wUbp+PrAgIoLkFuEZAJKGAa8C/liIwo+Yp3EwMzOzAdBnyErHWF0K3AssA+6MiKWSrpN0btrtFmCcpBXAp4CuaR5uAoZLWkoS1r4VEUsKfRCHxSHLzMzMBkBtPp0iYj4wv1vb1TnrO0mma+i+X0dP7ZlqagLJIcvMzMyKqrpmfIdkMtKJEx2yzMzMrKiqL2SB58oyMzOzoqvOkOW5sszMzKzIqjNkNTfDqlWwb1/WlZiZmVmFqt6QtWcPrFuXdSVmZmZWoao3ZIFvGZpVAUm3Slov6fFetkvS1yStkLRE0oycbRdJ+nO6XNTT/mZmvXHIMrNKdxtw1iG2nw0cly6XAN8AkDQWuAZ4Jckjwq6RNKaolZpZRXHIMrOKFhEPAId6MP1s4PZIPASMlnQM8CbgFxGxKSKeBX7BocOamdkB8pqMtOIMHw5jxzpkmRnAJGBVzuu2tK239oL4p7uX8sSarYV6OzMroOkTR3LNW0/o9/tU55Us8FxZZlYwki6R1Cqptb29PetyzKxEVOeVLEjmylqxIusqzCx7q4HJOa+b0rbVwKxu7ff39AYRcTNwM0BLS0vk86GF+C3ZzEqbr2RFXudDM6tc84D3p98yfBWwJSLWAvcCb5Q0Jh3w/sa0zcwsL9V7Jau5GTo6YPNmGOMvDJlVKknfJ7kiNV5SG8k3BusAIuI/gPnAm4EVwA7gA+m2TZL+GViUvtV1EXGoAfRmZgeo7pAFydUshyyzihURc/rYHsDHe9l2K3BrMeoys8pX3bcLwYPfzczMrCgcshyyzMzMrAiqN2SNHw9DhjhkmZmZWVFUb8iSPFeWmZmZFU31hixI5spyyDIzM7MiqO6Q1dwMK1dmXYWZmZlVIIes9evhueeyrsTMzMwqjEMW+GqWmZmZFZxDFnhclpmZmRWcQxY4ZJmZmVnBVXfImjgRamocsszMzKzgqjtk1dZCU5NDlpmZmRVcdYcs8FxZZmZmVhR5hSxJZ0laLmmFpCt62D5Y0g/S7b+TNDVn24mSfitpqaQ/SGooYP3957myzMzMrAj6DFmSaoCbgLOB6cAcSdO7dfsQ8GxEHAt8Ffhium8tcAfw0Yg4AZgF7ClY9YXQ3AxtbdDZmXUlZmZmVkHyuZI1E1gREU9GxG5gLjC7W5/ZwLfT9buAMyUJeCOwJCIeA4iIjRGxtzClF0hzM+zdC2vWZF2JmZmZVZB8QtYkYFXO67a0rcc+EdEJbAHGAS8CQtK9khZLurz/JReYp3EwMzOzIij2wPda4BTgPenP8ySd2b2TpEsktUpqbW9vL3JJ3ThkmZmZWRHkE7JWA5NzXjelbT32ScdhjQI2klz1eiAiNkTEDmA+MKP7B0TEzRHREhEtjY2Nh38U/TFlSvLTIcvMzMwKKJ+QtQg4TtI0SfXAhcC8bn3mARel6+cDCyIigHuBl0kamoav04AnClN6gQwZAhMmOGSZmZlZQdX21SEiOiVdShKYaoBbI2KppOuA1oiYB9wCfEfSCmATSRAjIp6V9BWSoBbA/Ij4aZGO5ch5riwzMzMrsD5DFkBEzCe51ZfbdnXO+k7ggl72vYNkGofS1dwMS5dmXYWZmZlVEM/4DknIevppiMi6EjMzM6sQDlmQhKznnoMNG7KuxMzMzCqEQxZ4GgczMzMrOIcscMgyMzOzgnPIAocsMzMzKziHLIDRo2HECIcsMzMzKxiHLADJc2WZmZlZQTlkdWluhpUrs67CzMzMKoRDVpeuubLMzMzMCsAhq0tzM2zaBB0dWVdiZmZmFcAhq4u/YWhmZmYF5JDVxSHLzMzMCsghq4tDlpmZmRWQQ1aXo4+G+nqHLDMzMysIh6wugwbB5MkOWWZmZlYQDlm5pkzxXFlmZmZWEA5ZuTxXlpmZmRWIQ1au5mZYswZ27866EjMzMytzDlm5mpshAtrasq7EzMzMypxDVi5P42BWcSSdJWm5pBWSruhhe7Ok+yQtkXS/pKacbV+StFTSMklfk6SBrd7MyplDVi6HLLOKIqkGuAk4G5gOzJE0vVu3G4DbI+JE4Drg8+m+rwFeC5wIvBR4BXDaAJVuZhXAISvX5MkgOWSZVY6ZwIqIeDIidgNzgdnd+kwHFqTrC3O2B9AA1AODgTrgmaJXbGYVwyErV309HHOMQ5ZZ5ZgErMp53Za25XoMeHu6fh4wQtK4iPgtSehamy73RsSynj5E0iWSWiW1tre3F/QAzKx8OWR157myzKrNZcBpkh4huR24Gtgr6VjgeKCJJJidIel1Pb1BRNwcES0R0dLY2DhQdZtZiXPI6s5zZZlVktXA5JzXTWnbfhGxJiLeHhEnA1elbZtJrmo9FBEdEdEB3AO8ekCqNrOK4JDVXXNzciVr376sKzGz/lsEHCdpmqR64EJgXm4HSeMldZ0LrwRuTddXklzhqpVUR3KVq8fbhWZmPXHI6q65OZmM9BmPbzUrdxHRCVwK3EsSkO6MiKWSrpN0btptFrBc0p+Ao4Dr0/a7gL8AfyAZt/VYRNw9kPWbWXmrzbqAkpM7jcMxx2Rbi5n1W0TMB+Z3a7s6Z/0ukkDVfb+9wEeKXqCZVay8rmTlMZnfYEk/SLf/TtLUbtunSOqQdFmB6i4ez5VlZmZmBdBnyMpzMr8PAc9GxLHAV4Evdtv+FZJBo6XPIcvMzMwKIJ8rWflM5jcb+Ha6fhdwZtfjJyS9DfgrsLQgFRfbiBEwZoxDlpmZmfVLPiErn8n89vdJB5puAcZJGg78A/BPh/qAkpvIz3NlmZmZWT8V+9uF1wJfTeeY6VXJTeTnubLMzMysn/L5dmGfk/nl9GmTVAuMAjYCrwTOl/QlYDSwT9LOiPh6fwsvquZmuP/+rKswMzOzMpZPyNo/mR9JmLoQeHe3PvOAi4DfAucDCyIigP2PoJB0LdBR8gELkpC1dSts3gyjR2ddjZmZmZWhPm8X5jmZ3y0kY7BWAJ8CDprmoaz4G4ZmZmbWT3lNRprHZH47gQv6eI9rj6C+bOSGrJe/PNtazMzMrCz5sTo98ZUsMzMz6yeHrJ40NkJDg0OWmZmZHTGHrJ5InivLzMzM+sUhqzeeK8vMzMz6wSGrNw5ZZmZm1g8OWb1pboZnnoGdO7OuxMzMzMqQQ1Zvur5h6HFZZmZmdgQcsnrjaRzMzMysHxyyeuOQZWZmZv3gkNWbSZNg0CCHLDMzMzsiDlm9qa1NgpbHZJmZmdkRcMg6FE/jYGZmZkfIIetQHLLMzMzsCDlkHUpzM7S1wd69WVdiZmZmZcYh61Cam6GzE9asyboSMzMzKzMOWYfiaRzMzMzsCDlkHYpDlpmZmR0hh6xDmTIl+emQZWZmZofJIetQhg6F8eM9V5aZmZkdNoesvngaBzMzMzsCDll9ccgyMzOzI+CQ1ZeukBWRdSVmZmZWRhyy+tLcDDt2wMaNWVdiZmZmZcQhqy+exsHMzMyOgENWX6ZOTX7+4Ae+ZWhmZmZ5c8jqy4knwrvfDV/+Mrz97bBlS9YVmZmZWRlwyOrLoEFwxx3w1a/C3XfDzJmwdGnWVZmZmVmJyytkSTpL0nJJKyRd0cP2wZJ+kG7/naSpafsbJD0s6Q/pzzMKXP/AkOCTn4SFC5MrWTNnJrcPzczMzHrRZ8iSVAPcBJwNTAfmSJrerduHgGcj4ljgq8AX0/YNwFsj4mXARcB3ClV4Jl73Oli8GE4+GS68ED71KdizJ+uqzMzMrATlcyVrJrAiIp6MiN3AXGB2tz6zgW+n63cBZ0pSRDwSEWvS9qXAEEmDC1F4ZiZOhAUL4BOfSG4hnnkmrFuXdVVmZmZWYvIJWZOAVTmv29K2HvtERCewBRjXrc87gMURsevISi0h9fXwr/+ajNVqbYUZM+A3v8m6KjMzMyshAzLwXdIJJLcQP9LL9ksktUpqbW9vH4iSCuM974GHHkoeJD1rFvzbv3maB7MSk8eY0mZJ90laIul+SU0526ZI+rmkZZKe6BpvamaWj3xC1mpgcs7rprStxz6SaoFRwMb0dRPw38D7I+IvPX1ARNwcES0R0dLY2Hh4R5C1E09MrmadfXZyC/F974Pt27OuyszIe0zpDcDtEXEicB3w+ZxttwNfjojjSYZOrC9+1WZWKfIJWYuA4yRNk1QPXAjM69ZnHsnAdoDzgQUREZJGAz8FroiIyr2fNno0/PjH8LnPwfe+B69+NaxYkXVVZpbfmNLpwIJ0fWHX9jSM1UbELwAioiMidgxM2WZWCfoMWekYq0uBe4FlwJ0RsVTSdZLOTbvdAoyTtAL4FNB1Sf5S4FjgakmPpsuEgh9FKRg0CK66Cu65B1avhpaWZF4tM8tSPmNKHwPenq6fB4yQNA54EbBZ0o8kPSLpy+mVMTOzvOQ1Jisi5kfEiyLihRFxfdp2dUTMS9d3RsQFEXFsRMyMiCfT9s9FxLCIOClnKdjl9n37CvVOBfSmN8HDD8MLXwjnnguf/Szs3Zt1VWbWu8uA0yQ9ApxGMvxhL1ALvC7d/grgBcDFPb1B2Y4rNbOiKusZ3885B6ZPh/e+F77yFbj/fti8OeuqSJ53+Otfwwc+kNxCfMtbYOPGrKsyq0Z9jimNiDUR8faIOBm4Km3bTHLV69H0VmMn8GNgRk8fUtbjSs2saGqzLqA/Xv/6JFjdfz9897vPt7/whcmsCl3LySfDgJ/3hgyBW25Jxmddemly+/CHP0wKMrOBsn9MKUm4uhB4d24HSeOBTRGxD7gSuDVn39GSGiOiHTgDaB2wys2s7JV1yPrUp5IFYP16eOSRZEL2xYuTO3b/9V/P921qOjB4zZiRzCsqFbFACT78YXj5y+H88+E1r4FvfCO5wmVmRRcRnZK6xpTWALd2jSkFWtMhD7OAz0sK4AHg4+m+eyVdBtwnScDDwP/N4jjMrDwpSmxep5aWlmhtLcwvi88+C48++nzwWrwYli9/fiqrCRMODl5TpxYpeLW3w5w5cN99cMkl8LWvweDynvzerFAkPRwRLVnXUQiFPIeZWek71PmrrK9k9WXMGDj99GTp0tEBjz12YPD65S+hszPZPnr0wbcajzsOavr7naLGRvjZz5KB8F/4QnLZ7Yc/hMmT+97XzMzMyk5Fh6yeDB8Or31tsnTZuRMef/zA4PVv/wa70gcADR2a3FpsbDz0MmFC8rOhoZcPr62Fz38eZs6Eiy5KUtzcucnzD83MzKyiVF3I6klDQzIuvSXnYt+ePbBsWRK4HnsseQZ0ezs89RQsWpSsd1396m748L4C2Xk03jKDxs98mAlveBvDPv+PcPnlRR4gZmZmZgPJIasXdXXJE3NOPLHn7RHJdBHt7Yde1qxJQlp7+/NXxhLNwM8BGHLFDhr/aQNTThrDaWfUcvrpyRj5IUOKfJBmZmZWNA5ZR0hKxnyNGQMvelHf/SOS8WAHBbH1QfvPnqD9V0v5Y+tL+cJDJ3H99TXU1weveY04/XQ444zkDmN9ffGPy8zMzArDIWuASDBiRLK84AUHbIF/aIFf74R/+RxbFz7Mr7e8lAW7z2Dhb8/m2vuP55prBjF0aHDKKc+HrhkzkiFeZmZmVpr8z3SpOOUUOOUURu7dy5sfe4w3L1gACy9n06/+wAPbZ7BgxxksfOBsrvz5cQCMHBGcetrzoevEE5PHJ5qZmVlpcMgqNTU1z88fcdlljN2zh7c9/DBvW7AAFnyM9b/+E/fvehULtp3Jwl+cxU9+0gzA2LHBrFnPh67jj/c4ejMzsyxV9GSkFWnXLnjoIViwABYuZPVvV7Kw8xQW6PUsqH8TT+86BoCjjgpOP/350PXCFzp0WenyZKRmVq4Odf5yyCp327fDb34DCxfCggX8ddEGFsZpLBj0BhbWvoE1u8cD0NQUnHFGErpe9jKYNCmZ18u3GK0UOGSZWbmq2hnfq8KwYfDGNyYLMG3LFqY9+CAfXLCAuO+L/GnJcyzkdBasfSPzv38mt98+av+utbXBxIkwaZKYNIleF08lYWZmdvgcsirNqFFwzjlwzjkIePGGDbz4V7/iowsXsO++z/LEH8UKjmU1k1jdOYnVKyexeu1UHl88mXs7j2Lb3mEHveWYkZ1J4Jpcw6SmngPZ+PG+KmZmZpbLIavSjR8P73gHvOMdDAJe+swzvPSvf02msF+3Dp55CtY9tP/11jUdrF5Xw+rd45MgxiRWb02XZZNYMmgy6/ZNIDgwUdXX7uWY8XtomriPSc21HDO5jpGjxPDhybQVw4cfemlo8JgxMzOrLA5Z1eaoo5KlFyOBkREcv23b80Fs3TpYtxKeWQTr1tG5Zj3r2jqTMLZpCKv3HZ1cFVs3ibZ1TTyyeBL3cDTbGJl3WTU1kQYu9RrEegtrXfOPdS0jRyY/PY+YmZllyf8M2cGkJKmMHNnjdPa1QFO6sG8fbNqUE8ZWwzOLYd069m3YxHPrt9GxfgcdG3fR8eweOrbspSOG0sFwtjGCDoYny97hdGwdQceucXRsHUtH7Sg6Bo1kHSPoiKFs6xxCx57BdOyqY9++/C55NTQcHMB6W7qCWW9LfX0S2nxL1MzM8uWQZf0zaFByS3L8eHjpSw/cBAxLl/3Xzvbtgy1bYONG2LAh+bl/aYcNy7q1pf3SBz8GsJOG/eFsGyPYVjOGbQ2NbBs8jm3149hWO5atNWPYVjOKbYxM+mwdxrZnh9G+dwhP7mlg667BbNtVT8fOusM6XCkJW11LTc2Br3try6e9vj4ZUjd69PM/c9dz2xoajvhvzMzMBohDlg2sQYOef+jjscfmt08E7NgBGzeijRsZsnEjQzZsoHHjxuQp3du3J0tHB2x/EjqWpOvbD/zZ0QF79hzw1vsQ2xmWBLHui0axdXAj2+rGsqduKJ11Q9hb10BnbbrUDN6/7K2pp3NQ11JHp+rYqzo6VUsndXRSS2fU0Lm7lt27B7EjaujcKzo7RWcndHYmOXLLlmTZu/fQfyT19b0HsO5t3bf1FtB6GhOXb1tP7V1XAM3MqpVDlpU+KZmqYtgwmDKlf++1e/cBoWxQRwcjtm9nRG+hbPt26Hgq+bljx4E/t2+HrTnrfSWj7mpqYOjQ5LiGDoVhQ2H8EGJwA9vrRrOlZiybB41li0azWWPYHKPYsm8Em/eOYEvnMDbvGcaWPUPY/NwQNm9uYPUf69m8o54tO+rYsbOmf39OBfDjH8Ps2VlXYWaWHYcsqy719ckyZkzh33v37gMDWPdQls+2XbvQc88x/Ll2hu9cxaSdO6GnpQ97qGULo9jMaDYz+oD13dRDzYH3LKO2Llmvq4V0PfZvrzu4X20v++csLzvqaGBC4f+czczKhEOWWaF0BbjRo4v7ORHJvcXeAtjOndQ99xzjd+5kfPdtzz2X7Lt/2d7tdfq+3du27jq4bffuQ9f51v8HnFvcPwszsxLmkGVWbqRkYFXWo98jkqDVPXx1LdOmZVufmVnGHLLM7MhIMHhwspiZ2UE864+ZmZlZEeQVsiSdJWm5pBWSruhh+2BJP0i3/07S1JxtV6btyyW9qYC1m5mZmZWsPkOWpBrgJuBsYDowR9L0bt0+BDwbEccCXwW+mO47HbgQOAE4C/j39P3MzMzMKlo+V7JmAisi4smI2A3MBbrPfjMb+Ha6fhdwpiSl7XMjYldE/BVYkb6fmZmZWUXLJ2RNAlblvG5L23rsExGdwBZgXJ77mpmZmVWckhj4LukSSa2SWtvb27Mux8zMzKzf8glZq4HJOa+b0rYe+0iqBUYBG/Pcl4i4OSJaIqKlsbEx/+rNzMzMSlQ+IWsRcJykaZLqSQayz+vWZx5wUbp+PrAgIiJtvzD99uE04Djg94Up3czMzKx09TkZaUR0SroUuBeoAW6NiKWSrgNaI2IecAvwHUkrgE0kQYy0353AE0An8PGIOMyn6JqZmZmVn7xmfI+I+cD8bm1X56zvBC7oZd/rgev7UaOZmZlZ2VFyV690SGoHnj6MXcYDG4pUzkDzsZSuSjqeUjyW5oioiAGZh3kOK8W/i/6opOPxsZSmUjyWXs9fJReyDpek1ohoybqOQvCxlK5KOp5KOpZyV2l/F5V0PD6W0lRux1ISUziYmZmZVRqHLDMzM7MiqISQdXPWBRSQj6V0VdLxVNKxlLtK+7uopOPxsZSmsjqWsh+TZWZmZlaKKuFKlpmZmVnJKduQJeksScslrZB0Rdb19IekyZIWSnpC0lJJf591Tf0lqUbSI5J+knUt/SFptKS7JP1R0jJJr866piMl6X+n/309Lun7khqyrqmaVco5zOev0uZzWLbKMmRJqgFuAs4GpgNzJE3Ptqp+6QQ+HRHTgVcBHy/z4wH4e2BZ1kUUwL8CP4uIlwAvp0yPSdIk4BNAS0S8lOTpDRdmW1X1qrBzmM9fpc3nsAyVZcgCZgIrIuLJiNgNzAVmZ1zTEYuItRGxOF3fRvI/waRsqzpykpqAtwD/mXUt/SFpFHAqyWOjiIjdEbE506L6pxYYkj7EfSiwJuN6qlnFnMN8/ipdPodlr1xD1iRgVc7rNsr4f+pckqYCJwO/y7iU/rgRuBzYl3Ed/TUNaAe+ld46+E9Jw7Iu6khExGrgBmAlsBbYEhE/z7aqqlaR5zCfv0qOz2EZK9eQVZEkDQd+CHwyIrZmXc+RkHQOsD4iHs66lgKoBWYA34iIk4HtQFmOnZE0huRKyTRgIjBM0nuzrcoqic9fJcnnsIyVa8haDUzOed2UtpUtSXUkJ6jvRsSPsq6nH14LnCvpKZJbIGdIuiPbko5YG9AWEV2/ld9FcsIqR68H/hoR7RGxB/gR8JqMa6pmFXUO8/mrZPkclrFyDVmLgOMkTZNUTzL4bV7GNR0xSSK5Z74sIr6SdT39ERFXRkRTREwl+XtZEBEl/9tGTyJiHbBK0ovTpjOBJzIsqT9WAq+SNDT97+1MynQAbIWomHOYz1+ly+ew7NVmXcCRiIhOSZcC95J8w+DWiFiacVn98VrgfcAfJD2atn0mIuZnV5Kl/g74bvoP4ZPABzKu54hExO8k3QUsJvk22COU2czJlaTCzmE+f5U2n8My5BnfzczMzIqgXG8XmpmZmZU0hywzMzOzInDIMjMzMysChywzMzOzInDIMjMzMysChywzMzOzInDIMjMzMysChywzMzOzIvj/vY9OC8SNMbQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].plot(train_loss, c=\"red\" , label=\"Train\")\n",
    "ax[0].plot(valid_loss, c=\"blue\", label=\"Valid\")\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(valid_acc)\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\son22\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = np.zeros((classes, classes))\n",
    "with torch.no_grad():\n",
    "    train_model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = train_model(inputs)\n",
    "        loss = ((output - targets.float())**2).mean()\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions))\n",
    "        \n",
    "        confusion_matrix += compute_confusion_matrix(targets, predictions)\n",
    "\n",
    "    test_accuracy = np.mean(test_accuracies)\n",
    "    \n",
    "    train_model.train()\n",
    "print(f\"Test accuracy: {test_accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize(matrix, axis):\n",
    "    axis = {'true': 1, 'pred': 0}[axis]\n",
    "    return matrix / matrix.sum(axis=axis, keepdims=True)\n",
    "\n",
    "x_labels = list(range(classes))\n",
    "y_labels = x_labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(\n",
    "    ax=plt.gca(),\n",
    "    data=normalize(confusion_matrix, 'true'),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cmap=\"Reds\",\n",
    "    cbar=False,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=x_labels,\n",
    "    yticklabels=y_labels,\n",
    ")\n",
    "# plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
